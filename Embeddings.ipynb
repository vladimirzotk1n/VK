{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Данный ноутбук использовал окружение google-colab\n",
        "%pip install catboost fasttext -q"
      ],
      "metadata": {
        "id": "rwlxK5AYASaT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание \"NLP. Часть 1\""
      ],
      "metadata": {
        "id": "3xRUXhCVUzur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koQiHQFT8XO7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "ZUhaEvmpTCsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_pretokenize_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words"
      ],
      "metadata": {
        "id": "_q88wy8uTDZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block is for tests only\n",
        "test_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"never jump over the lazy dog quickly\",\n",
        "    \"brown foxes are quick and dogs are lazy\"\n",
        "]\n",
        "\n",
        "def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        all_words.extend(words)\n",
        "    vocab = sorted(set(all_words))\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab, vocab_index\n",
        "\n",
        "vocab, vocab_index = build_vocab(test_corpus)"
      ],
      "metadata": {
        "id": "uGDzAEpJT_zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "Реализовать One-Hot векторизацию текстов"
      ],
      "metadata": {
        "id": "eemkFZ1tVLw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_vectorization(\n",
        "    text: str,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> List[int]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "    embedding = np.zeros(len(vocab))\n",
        "    for word in tokens:\n",
        "        if word in vocab:\n",
        "            embedding[vocab_index[word]] = 1\n",
        "\n",
        "    return embedding.tolist()\n",
        "\n",
        "def test_one_hot_vectorization(\n",
        "    vocab: List[str],\n",
        "    vocab_index: Dict[str, int]\n",
        ") -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown fox\"\n",
        "        result = one_hot_vectorization(text, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        words_in_text = normalize_pretokenize_text(text)\n",
        "        for word in words_in_text:\n",
        "            if word in vocab_index:\n",
        "                idx = vocab_index[word]\n",
        "                if result[idx] != 1:\n",
        "                    return False\n",
        "\n",
        "        print(\"One-Hot-Vectors test PASSED\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"One-Hot-Vectors test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "Qiw7w5OhTDeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_one_hot_vectorization(vocab, vocab_index)"
      ],
      "metadata": {
        "id": "Q2-LJcmbTe04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65e685d-36f3-4299-d5f2-6269c25be2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot-Vectors test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2 (0.5 балла)\n",
        "Реализовать Bag-of-Words"
      ],
      "metadata": {
        "id": "hAF8IOYMVT3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words_vectorization(text: str) -> Dict[str, int]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "    counter = Counter(tokens)\n",
        "    return counter\n",
        "\n",
        "def test_bag_of_words_vectorization() -> bool:\n",
        "    try:\n",
        "        text = \"the the quick brown brown brown\"\n",
        "        result = bag_of_words_vectorization(text)\n",
        "\n",
        "        if not isinstance(result, dict):\n",
        "            return False\n",
        "\n",
        "        if result.get('the', 0) != 2:\n",
        "            return False\n",
        "        if result.get('quick', 0) != 1:\n",
        "            return False\n",
        "        if result.get('brown', 0) != 3:\n",
        "            return False\n",
        "        if result.get('nonexistent', 0) != 0:\n",
        "            return False\n",
        "\n",
        "        print(\"Bad-of-Words test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Bag-of-Words test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "-_QjiviNBkbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_bag_of_words_vectorization()"
      ],
      "metadata": {
        "id": "ScFuXh_9TtJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c97776-689d-4880-87f8-3d8b9f2d9c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad-of-Words test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3 (0.5 балла)\n",
        "Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "d6LblWJfX2kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf_vectorization(text: str, corpus: List[str] = None, vocab: List[str] = None, vocab_index: Dict[str, int] = None) -> List[float]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "    tf_counter = Counter(tokens)\n",
        "    tfs = [tf_counter.get(word, 0) / len(tokens) for word in vocab]\n",
        "\n",
        "    idf_counter = Counter()\n",
        "    for doc in corpus:\n",
        "        doc_tokens = set(normalize_pretokenize_text(doc))\n",
        "        idf_counter.update(doc_tokens)\n",
        "    idfs = [math.log(len(corpus) / (idf_counter.get(word, 0) + 1e-9)) for word in vocab]\n",
        "    return [tf * idf for tf, idf in zip(tfs, idfs)]\n",
        "\n",
        "def test_tf_idf_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown\"\n",
        "        result = tf_idf_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"TF-IDF test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"TF-IDF test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "RqcMYJkrTlV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_tf_idf_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "GKIyS724T0XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130f06e3-07be-4f2c-a822-f3314e942fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4 (1 балл)\n",
        "Реализовать Positive Pointwise Mutual Information (PPMI).  \n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
        "$$PPMI(word, context) = max(0, PMI(word, context))$$\n",
        "$$PMI(word, context) = log \\frac{P(word, context)}{P(word) P(context)} = log \\frac{N(word, context)|(word, context)|}{N(word) N(context)}$$\n",
        "где $N(word, context)$ -- число вхождений слова $word$ в окно $context$ (размер окна -- гиперпараметр)"
      ],
      "metadata": {
        "id": "T0f9FZCrX5_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "\n",
        "def ppmi_vectorization(text: str, corpus: List[str], vocab: List[str], vocab_index: Dict[str, int], window_size: int = 2) -> np.ndarray:\n",
        "    V = len(vocab)\n",
        "    if not corpus or not vocab or not vocab_index:\n",
        "        return np.zeros(V, dtype=np.float32)\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    for doc in corpus:\n",
        "        tokens = normalize_pretokenize_text(doc)\n",
        "        n = len(tokens)\n",
        "        for i, center_word in enumerate(tokens):\n",
        "            if center_word not in vocab_index:\n",
        "                continue\n",
        "            center_idx = vocab_index[center_word]\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(n, i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                context_word = tokens[j]\n",
        "                if context_word not in vocab_index:\n",
        "                    continue\n",
        "                context_idx = vocab_index[context_word]\n",
        "                rows.append(center_idx)\n",
        "                cols.append(context_idx)\n",
        "                data.append(1.0)\n",
        "\n",
        "    if not data:\n",
        "        return np.zeros(V, dtype=np.float32)\n",
        "\n",
        "    cooc_mat = coo_matrix((data, (rows, cols)), shape=(V, V), dtype=np.float32).tocsr()\n",
        "\n",
        "    total = cooc_mat.sum()\n",
        "    if total == 0:\n",
        "        return np.zeros(V, dtype=np.float32)\n",
        "\n",
        "    p_w = np.array(cooc_mat.sum(axis=1)).flatten() / total\n",
        "    p_c = np.array(cooc_mat.sum(axis=0)).flatten() / total\n",
        "\n",
        "    cooc_mat = cooc_mat.tocoo()\n",
        "    p_wc = cooc_mat.data / total\n",
        "    denom = p_w[cooc_mat.row] * p_c[cooc_mat.col] + 1e-12\n",
        "    ppmi_data = np.maximum(0, np.log2(p_wc / denom))\n",
        "    ppmi_mat = csr_matrix((ppmi_data, (cooc_mat.row, cooc_mat.col)), shape=(V, V), dtype=np.float32)\n",
        "\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "    valid_indices = [vocab_index[w] for w in tokens if w in vocab_index]\n",
        "    if not valid_indices:\n",
        "        return np.zeros(V, dtype=np.float32)\n",
        "\n",
        "    vector = ppmi_mat[valid_indices].mean(axis=0).A1\n",
        "    return vector.astype(np.float32).tolist()\n",
        "\n",
        "def test_ppmi_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"quick brown fox\"\n",
        "        result = ppmi_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"PPMI test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"PPMI test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "SUg6K2-wTwr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_ppmi_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "HgHmNZy75XFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d940d52-6c4e-41fe-899e-23452268ec7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 5 (1 балл)\n",
        "Реализовать получение эмбеддингов из fasttext и bert (для bert лучше использовать CLS токен)"
      ],
      "metadata": {
        "id": "FK29va3PBH_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\n",
        "# !unzip wiki.en.zip"
      ],
      "metadata": {
        "id": "L5UeV2L8h0zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.load_model('wiki.en.bin')\n",
        "\n",
        "def get_fasttext_embeddings(text: str, model_path: str = None, model: any = model) -> List[np.ndarray]:\n",
        "    tokens = normalize_pretokenize_text(text)\n",
        "    embeddings = []\n",
        "    for word in tokens:\n",
        "        emb = model.get_word_vector(word)\n",
        "        embeddings.append(emb)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "tOe8dRLl5eqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_fasttext_embeddings(\"  my friends\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BBpstSkmCRw",
        "outputId": "359d850c-6fb0-4e81-9b81-031a58bbcb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-1.26858622e-01,  1.52882934e-01,  1.49032772e-01,  3.92690450e-02,\n",
              "        -1.30524158e-01, -3.80693674e-02, -1.62300408e-01, -2.76634097e-03,\n",
              "         1.21185914e-01,  1.42024562e-01,  4.11207974e-01, -2.44587511e-01,\n",
              "        -9.64097083e-02,  2.38146663e-01,  8.28057975e-02, -1.26340926e-01,\n",
              "         6.91213086e-02,  8.52295905e-02, -7.36847520e-04,  7.16888234e-02,\n",
              "        -8.18991661e-02,  1.00517511e-01, -1.16089121e-01, -7.65723884e-02,\n",
              "        -1.08819604e-02,  9.11623016e-02, -1.83051862e-02,  8.34907368e-02,\n",
              "        -2.89088726e-01,  2.27737129e-01, -7.89996535e-02,  3.03535312e-01,\n",
              "        -1.69834763e-01,  1.47010624e-01, -5.41614830e-01, -1.04691982e-02,\n",
              "        -1.70235902e-01, -1.41128808e-01, -1.41191676e-01, -3.36246461e-01,\n",
              "         2.03713253e-01, -1.27543211e-02, -9.01129246e-02,  6.05605021e-02,\n",
              "         2.20914185e-01,  1.35715351e-01,  2.11866438e-01,  1.99898273e-01,\n",
              "        -3.12961221e-01, -1.15871273e-01, -8.84860605e-02, -5.90643644e-01,\n",
              "        -2.59140581e-02, -9.19335037e-02, -1.01218283e-01,  3.78924638e-01,\n",
              "        -3.55740070e-01, -2.96334058e-01, -5.22761494e-02,  2.82708436e-01,\n",
              "         6.96739554e-03,  6.41093999e-02,  1.63446307e-01,  1.27662539e-01,\n",
              "        -1.17070526e-02, -3.27853948e-01,  3.17711115e-01,  2.20291868e-01,\n",
              "         3.05482566e-01,  1.38384700e-01,  2.84632623e-01, -1.38319418e-01,\n",
              "         1.53968930e-01, -1.50333747e-01, -2.41531223e-01,  6.99668825e-02,\n",
              "         3.12442631e-02,  3.22322428e-01, -1.41965345e-01,  1.27406672e-01,\n",
              "        -7.54199922e-02,  4.96991664e-01,  1.67758346e-01,  3.53048742e-02,\n",
              "         1.79039493e-01,  1.21034540e-01,  6.44437820e-02,  2.27593184e-02,\n",
              "        -3.18862259e-01, -1.21340446e-01,  2.44325504e-01, -5.98094091e-02,\n",
              "         2.58536249e-01, -1.64239854e-01,  1.35081172e-01,  1.34156257e-01,\n",
              "        -3.76614243e-01, -9.80154946e-02,  2.30602324e-02,  2.31017992e-01,\n",
              "         7.59624243e-02, -1.04057878e-01,  1.26759112e-01, -9.87385511e-02,\n",
              "        -1.62970334e-01, -2.90516168e-01,  8.66022334e-02,  3.48823458e-01,\n",
              "         1.94100440e-01,  1.43777788e-01,  1.30509481e-01,  1.23177186e-01,\n",
              "         6.32807016e-02,  7.91000128e-02, -3.09758067e-01, -1.20858550e-02,\n",
              "         6.71058595e-02, -1.18768781e-01, -2.27147266e-01,  2.34980226e-01,\n",
              "         4.40538436e-01,  2.21194267e-01, -8.21175799e-02,  5.49915075e-01,\n",
              "         1.67066276e-01,  8.23182464e-02, -8.58657286e-02,  5.95077798e-02,\n",
              "         1.01868346e-01,  3.38358879e-01,  3.97432834e-01,  3.64580542e-01,\n",
              "        -5.80777377e-02, -1.64795786e-01,  2.27369159e-01, -2.60172337e-01,\n",
              "        -3.41731489e-01,  8.00200626e-02,  4.21452463e-01,  7.75502399e-02,\n",
              "        -1.17094137e-01,  8.82372260e-05,  4.88311052e-02, -6.69416785e-02,\n",
              "        -1.50726825e-01, -1.64454877e-02, -4.17747498e-02, -3.20769012e-01,\n",
              "         5.48740178e-02,  2.63966978e-01,  4.14436877e-01,  1.58508793e-02,\n",
              "         2.49404624e-01, -1.90433651e-01,  7.30341822e-02,  3.16049278e-01,\n",
              "        -3.78512263e-01, -3.98418307e-02,  1.51359141e-02,  2.37564757e-01,\n",
              "        -2.12534964e-02,  1.10901341e-01, -2.10914329e-01, -1.93980545e-01,\n",
              "        -2.13739350e-01,  4.78381574e-01,  2.06910670e-02, -2.78956532e-01,\n",
              "         4.07067388e-02,  5.37502319e-02, -1.47957742e-01, -2.00999171e-01,\n",
              "        -3.67182285e-01, -8.23493302e-02, -7.28850067e-03,  3.29064429e-01,\n",
              "         1.32801980e-01,  9.16739106e-02, -9.77039412e-02, -4.27116275e-01,\n",
              "         1.85634851e-01, -2.38364488e-01,  1.77219614e-01, -6.98371232e-02,\n",
              "         2.88439952e-02,  2.60762572e-01,  2.67467022e-01, -6.75895393e-01,\n",
              "         1.29329503e-01, -6.02625906e-02,  1.96474180e-01, -1.33247182e-01,\n",
              "        -5.17967083e-02,  6.40025139e-01, -2.58047134e-01, -5.64370900e-02,\n",
              "        -3.49104196e-01, -5.65495342e-02, -4.18193698e-01, -1.56090379e-01,\n",
              "         3.37449700e-01,  2.70697832e-01,  6.70849308e-02, -7.85022154e-02,\n",
              "        -6.32167161e-02,  9.87200662e-02, -3.03124040e-01, -8.25878978e-03,\n",
              "         2.34575123e-02, -3.18817854e-01,  3.30177933e-01,  1.41701341e-01,\n",
              "         2.44375274e-01, -4.13343519e-01, -1.85342774e-01, -3.26912969e-01,\n",
              "        -1.24165744e-01, -1.81981549e-01, -2.75810987e-01,  3.12049717e-01,\n",
              "        -2.46441439e-01, -1.82779908e-01, -8.04365426e-02, -8.07903856e-02,\n",
              "         2.62856185e-02,  1.72711998e-01, -3.20949674e-01, -9.36535001e-03,\n",
              "        -1.80529095e-02, -3.75611335e-03, -8.35011601e-02,  1.09198570e-01,\n",
              "        -7.44292140e-03, -1.70296356e-01, -6.15480542e-02, -4.80348542e-02,\n",
              "         3.63523215e-02, -6.17071427e-02, -7.40456581e-03,  3.04571651e-02,\n",
              "         3.31526399e-02,  1.13805175e-01,  2.53781199e-01,  2.31037408e-01,\n",
              "        -1.40518665e-01,  2.36207545e-01,  2.69963324e-01, -8.10362995e-02,\n",
              "        -1.15669906e-01, -1.45049393e-01, -6.05987012e-03, -3.93372953e-01,\n",
              "        -4.99044359e-03,  2.72398055e-01,  7.69494027e-02,  1.03690773e-02,\n",
              "        -2.64384031e-01, -1.06918156e-01,  4.51868951e-01,  1.57962024e-01,\n",
              "         2.26718783e-02,  1.35259286e-01, -4.78614271e-02,  7.31378794e-04,\n",
              "        -9.51265544e-03, -3.71518694e-02,  1.06978446e-01,  3.41029108e-01,\n",
              "        -2.47804970e-02, -3.72686028e-01, -2.23291814e-01, -2.47447610e-01,\n",
              "         5.85107923e-01,  5.36783226e-02, -8.46106410e-02, -1.33895814e-01,\n",
              "        -1.29099712e-01,  1.03491247e-01,  8.33237618e-02, -9.20630097e-02,\n",
              "         9.75769609e-02,  2.96814620e-01, -2.11363658e-01, -2.38287747e-02,\n",
              "         1.80070743e-01,  4.38007116e-02,  8.80543888e-02,  2.04653889e-01,\n",
              "        -2.96438605e-01,  1.74608007e-01,  6.28729463e-02,  1.49420783e-01,\n",
              "        -1.46163642e-01, -2.10857257e-01,  3.21351945e-01, -3.72577906e-02,\n",
              "        -6.03011213e-02,  4.19109464e-01,  3.28542367e-02, -1.23025812e-01],\n",
              "       dtype=float32),\n",
              " array([ 1.60773799e-01, -1.70993999e-01, -1.08153276e-01, -6.35864809e-02,\n",
              "        -2.12378278e-01,  1.67771861e-01,  1.28382713e-01, -2.18468979e-01,\n",
              "         1.65354475e-01,  5.24045289e-01,  2.08566442e-01, -1.12444246e-02,\n",
              "        -3.16987783e-01,  9.93300155e-02,  1.28115550e-01, -3.92128713e-03,\n",
              "         1.11965761e-01, -1.20933846e-01, -2.69139390e-02,  1.20689653e-01,\n",
              "        -9.01813656e-02,  2.62143314e-01, -3.09445839e-02, -3.42051052e-02,\n",
              "         2.34450921e-01,  1.88403025e-01, -1.41044958e-02,  2.26951912e-01,\n",
              "        -2.10569724e-01,  9.66125447e-03, -1.75192878e-01,  1.64223135e-01,\n",
              "        -2.03687564e-01, -3.23345572e-01, -3.30325328e-02, -9.30951610e-02,\n",
              "         1.46520004e-01, -3.85438949e-02, -1.00710303e-01, -1.08783498e-01,\n",
              "         3.46642494e-01,  1.06135607e-01,  5.40531836e-02,  6.99322820e-02,\n",
              "        -1.33275360e-01,  2.14667663e-01,  7.94826522e-02,  5.81613444e-02,\n",
              "        -2.27751791e-01, -5.68692088e-02, -7.43826926e-02, -5.55244386e-02,\n",
              "        -1.12627849e-01, -4.05173190e-02, -1.34498224e-01,  2.95987397e-01,\n",
              "         5.38213432e-01,  7.01458380e-02, -1.35899454e-01,  4.05130267e-01,\n",
              "         1.91645712e-01,  1.08878419e-01, -5.66173643e-02, -1.00032106e-01,\n",
              "        -2.32933730e-01, -1.17904665e-02, -1.53330430e-01,  4.73458588e-01,\n",
              "        -1.43369928e-01,  1.98978353e-02,  2.30392534e-02,  9.71356928e-02,\n",
              "         8.37189704e-02, -3.92212123e-01, -1.29928306e-01,  1.27362935e-02,\n",
              "         2.10989982e-01,  1.76991507e-01,  1.05890110e-01, -1.01574577e-01,\n",
              "        -9.93062779e-02, -1.02340989e-01, -1.84158176e-01,  2.62443453e-01,\n",
              "        -1.52505204e-01, -2.77985763e-02, -1.93250209e-01,  6.95318282e-02,\n",
              "         5.99124394e-02,  3.81919369e-02,  5.46556450e-02, -8.09377059e-02,\n",
              "        -1.80098489e-01,  5.59833832e-03,  4.30829734e-01,  3.91228944e-01,\n",
              "        -1.61266431e-01, -3.32943767e-01, -3.80895585e-01, -1.15622170e-01,\n",
              "         3.72036785e-01,  2.65419543e-01,  8.63748118e-02, -9.72541869e-02,\n",
              "        -2.87395239e-01, -9.07140821e-02, -7.48522431e-02,  2.36896247e-01,\n",
              "         4.86954808e-01,  1.52226940e-01, -7.11712763e-02, -1.83124423e-01,\n",
              "        -8.09892789e-02,  8.24916214e-02, -8.52925479e-02,  1.60851683e-02,\n",
              "         2.93470949e-01,  8.45444649e-02, -4.10772204e-01,  1.85753271e-01,\n",
              "         2.69694835e-01, -1.89948067e-01, -2.77017392e-02,  1.68843597e-01,\n",
              "         2.98579156e-01,  4.75591645e-02,  2.28723407e-01, -1.00568019e-01,\n",
              "        -1.66945130e-01,  3.36925477e-01,  7.53629133e-02, -1.18933372e-01,\n",
              "        -3.36006373e-01, -1.20623507e-01,  2.22541485e-02, -9.75105241e-02,\n",
              "        -2.51845747e-01,  4.44628112e-03, -2.13633537e-01,  1.19014159e-01,\n",
              "         2.22994372e-01,  1.01871759e-01,  2.20612679e-02, -1.20740257e-01,\n",
              "        -3.69512327e-02,  4.31445912e-02,  1.30269691e-01, -2.48698279e-01,\n",
              "         3.25807184e-02,  4.43556942e-02,  3.85108262e-01, -1.61230460e-01,\n",
              "        -2.13656202e-02, -7.04299286e-02, -1.27136901e-01, -4.86365892e-02,\n",
              "        -4.32794303e-01, -2.55299985e-01,  5.00682443e-02,  1.32527173e-01,\n",
              "         1.72405884e-01,  5.02073228e-01,  6.54166862e-02,  4.22602117e-01,\n",
              "        -7.83004537e-02,  5.16088903e-02,  1.32300019e-01,  8.83475393e-02,\n",
              "         1.96076334e-01, -4.65974398e-02, -1.39514497e-02,  4.38484410e-03,\n",
              "        -3.81376892e-01,  1.22065246e-01, -3.65172960e-02,  2.98019111e-01,\n",
              "         2.51546651e-01,  1.85926527e-01,  1.23642288e-01,  4.20869365e-02,\n",
              "        -1.47283673e-02,  1.27032042e-01,  7.88417533e-02,  4.20911126e-02,\n",
              "         4.70749773e-02,  1.67780295e-01, -2.41864026e-01,  2.28155538e-01,\n",
              "         1.36992589e-01,  5.09843007e-02, -2.75045633e-02, -1.77096635e-01,\n",
              "         2.14930907e-01,  2.52963662e-01, -2.82995552e-01,  3.37804317e-01,\n",
              "        -2.05461800e-01,  8.74739215e-02, -1.08686857e-01, -1.86345190e-01,\n",
              "         2.87144154e-01,  5.46601275e-03,  2.12147504e-01,  1.72391817e-01,\n",
              "        -5.67205474e-02, -6.81888834e-02, -3.09363276e-01, -1.11644948e-02,\n",
              "        -1.94329143e-01,  1.91630870e-01,  8.07371318e-01, -9.93577242e-02,\n",
              "         2.53534853e-01, -1.28054455e-01, -5.04125208e-02, -2.30661497e-01,\n",
              "         5.46033084e-02, -1.99118450e-01,  2.66526267e-02, -1.13688454e-01,\n",
              "        -8.44930708e-02,  5.96646555e-02, -2.02487260e-01, -8.66438895e-02,\n",
              "        -7.04590529e-02,  4.42591049e-02, -2.36994416e-01, -7.17243105e-02,\n",
              "         5.39068058e-02,  2.71003336e-01,  3.12794328e-01,  1.68113053e-01,\n",
              "         2.02360258e-01,  4.31623943e-02, -3.63027938e-02,  1.33419871e-01,\n",
              "         1.64367497e-01,  2.81534661e-02, -8.71839747e-02, -4.28931296e-01,\n",
              "        -6.83385413e-04, -3.05503547e-01, -2.06353869e-02,  1.71602607e-01,\n",
              "        -7.55890086e-02,  3.72979224e-01, -1.02208955e-02, -8.81800652e-02,\n",
              "        -5.32879643e-02, -2.31510252e-01, -5.21775857e-02, -1.94683656e-01,\n",
              "        -2.76339740e-01, -2.75452286e-01,  1.59732580e-01, -1.85678333e-01,\n",
              "        -8.32423642e-02, -1.33089706e-01,  3.28823477e-01, -1.11962527e-01,\n",
              "         1.55548140e-01,  7.32939020e-02,  2.42107213e-01,  4.13976647e-02,\n",
              "        -1.97060719e-01,  5.97656257e-02,  7.18398616e-02, -8.57242495e-02,\n",
              "         5.34777120e-02, -1.41571879e-01, -9.24990922e-02, -9.41549540e-02,\n",
              "        -1.87308695e-02,  2.65029132e-01, -1.78713515e-01,  5.65220341e-02,\n",
              "         5.47479354e-02,  1.38743222e-01, -6.91704154e-02,  2.02143073e-01,\n",
              "         1.10202961e-01, -4.10818867e-02,  1.75371066e-01,  4.07753736e-01,\n",
              "         1.55878752e-01,  1.55244336e-01, -6.67944103e-02,  1.17568441e-01,\n",
              "         3.05893093e-01, -2.69296318e-01,  7.43928030e-02, -2.12380990e-01,\n",
              "         1.82780042e-01, -3.60780396e-02, -7.08243623e-02,  1.67756617e-01,\n",
              "        -1.91179097e-01,  4.24253196e-01,  3.09818655e-01,  1.65653378e-01],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embeddings(\n",
        "    text: str,\n",
        "    model_name: str = 'bert-base-uncased',\n",
        "    pool_method: str = 'cls'\n",
        ") -> np.ndarray:\n",
        "    # tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    # model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    tokens = tokenizer(text, return_tensors='pt', max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "\n",
        "    return outputs['pooler_output'][0].detach().numpy()"
      ],
      "metadata": {
        "id": "A9GXy6n0AtsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18258f41-366c-4659-e2d9-dfb37756cca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 6 (1.5 балла)\n",
        "Реализовать обучение так, чтобы можно было поверх эмбеддингов, реализованных в предыдущих заданиях, обучить какую-то модель (вероятно неглубокую, например, CatBoost) на задаче классификации текстов ([IMDB](https://huggingface.co/datasets/stanfordnlp/imdb))."
      ],
      "metadata": {
        "id": "E_KoKolrD49R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "class Vectorizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = None\n",
        "        self.vocab_index = None\n",
        "\n",
        "    def vectorize_dataset(\n",
        "        self,\n",
        "        dataset_name: str = \"imdb\",\n",
        "        vectorizer_type: str = \"bow\",\n",
        "        split: str = \"train\",\n",
        "        sample_size: int = 100\n",
        "    ) -> Tuple[Any, List, List]:\n",
        "\n",
        "        dataset = datasets.load_dataset(dataset_name, split=split)\n",
        "        class_0 = dataset.filter(lambda x: x['label'] == 0).shuffle(42).select(range(sample_size//2))\n",
        "        class_1 = dataset.filter(lambda x: x['label'] == 1).shuffle(42).select(range(sample_size//2))\n",
        "        dataset = concatenate_datasets([class_0, class_1]).shuffle(42)\n",
        "\n",
        "        texts = [item['text'] for item in dataset if 'text' in item and item['text'].strip()]\n",
        "        labels = [item['label'] for item in dataset if 'label' in item]\n",
        "\n",
        "        def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "            all_words = []\n",
        "            for text in texts:\n",
        "                words = normalize_pretokenize_text(text)\n",
        "                all_words.extend(words)\n",
        "            vocab = sorted(set(all_words))\n",
        "            vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "            return vocab, vocab_index\n",
        "\n",
        "        if split == \"train\":\n",
        "            self.vocab, self.vocab_index = build_vocab(texts)\n",
        "\n",
        "        if split == \"test\" and self.vocab is None:\n",
        "            raise ValueError(\"Can not vectorize test data before train\")\n",
        "\n",
        "        vectorized_data = []\n",
        "        for text in texts:\n",
        "            if vectorizer_type == \"one_hot\":\n",
        "                vectorized_data.append(one_hot_vectorization(text, self.vocab, self.vocab_index))\n",
        "            elif vectorizer_type == \"bow\":\n",
        "                bow_dict = bag_of_words_vectorization(text)\n",
        "                vector = [bow_dict.get(word, 0) for word in self.vocab]\n",
        "                vectorized_data.append(vector)\n",
        "            elif vectorizer_type == \"tfidf\":\n",
        "                vectorized_data.append(tf_idf_vectorization(text, texts, self.vocab, self.vocab_index))\n",
        "            elif vectorizer_type == \"ppmi\":\n",
        "                vectorized_data.append(ppmi_vectorization(text, texts, self.vocab, self.vocab_index))\n",
        "            elif vectorizer_type == \"fasttext\":\n",
        "                embeddings = get_fasttext_embeddings(text)\n",
        "                if embeddings:\n",
        "                    avg_embedding = np.mean(embeddings, axis=0)\n",
        "                    vectorized_data.append(avg_embedding.tolist())\n",
        "                else:\n",
        "                    vectorized_data.append([0] * 300)\n",
        "            elif vectorizer_type == \"bert\":\n",
        "                embedding = get_bert_embeddings(text)\n",
        "                vectorized_data.append(embedding.tolist())\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown vectorizer type: {vectorizer_type}\")\n",
        "        return vocab, vectorized_data, labels"
      ],
      "metadata": {
        "id": "zsc98L8JE8G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "\n",
        "def train(\n",
        "    embeddings_method=\"bow\",\n",
        "    val_size=0.2,\n",
        "    cv_folds=5\n",
        "):\n",
        "\n",
        "    vocab, X, y = vectorizer.vectorize_dataset(\"imdb\", embeddings_method, \"train\")\n",
        "    _, X_test, y_test = vectorizer.vectorize_dataset(\"imdb\", embeddings_method, \"test\")\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y,\n",
        "        test_size=val_size,\n",
        "        shuffle=True,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=50,\n",
        "        learning_rate=0.1,\n",
        "        random_seed=42,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"<==========> Embedding method: {embeddings_method} <==========>\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    cv_scores = cross_val_score(\n",
        "        model,\n",
        "        X, y,\n",
        "        cv=cv_folds,\n",
        "        scoring='f1'\n",
        "    )\n",
        "    print(f\"Mean CV F1: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "id": "DRRw01XiBg6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for embeddings_method in [\"bow\", \"one_hot\", \"tfidf\", \"ppmi\", \"fasttext\", \"bert\"]:\n",
        "    train(embeddings_method=embeddings_method)"
      ],
      "metadata": {
        "id": "naMqAkjqFHAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d81ff54-ebb8-4a22-bfe2-92a057b7befb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<==========> Embedding method: bow <==========>\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.54      0.57        50\n",
            "           1       0.59      0.66      0.62        50\n",
            "\n",
            "    accuracy                           0.60       100\n",
            "   macro avg       0.60      0.60      0.60       100\n",
            "weighted avg       0.60      0.60      0.60       100\n",
            "\n",
            "Mean CV F1: 0.6070 ± 0.1117\n",
            "\n",
            "\n",
            "\n",
            "<==========> Embedding method: one_hot <==========>\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.60      0.63        50\n",
            "           1       0.64      0.70      0.67        50\n",
            "\n",
            "    accuracy                           0.65       100\n",
            "   macro avg       0.65      0.65      0.65       100\n",
            "weighted avg       0.65      0.65      0.65       100\n",
            "\n",
            "Mean CV F1: 0.5855 ± 0.1064\n",
            "\n",
            "\n",
            "\n",
            "<==========> Embedding method: tfidf <==========>\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.76      0.70        50\n",
            "           1       0.71      0.58      0.64        50\n",
            "\n",
            "    accuracy                           0.67       100\n",
            "   macro avg       0.68      0.67      0.67       100\n",
            "weighted avg       0.68      0.67      0.67       100\n",
            "\n",
            "Mean CV F1: 0.6502 ± 0.0665\n",
            "\n",
            "\n",
            "\n",
            "<==========> Embedding method: ppmi <==========>\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.20      0.27        50\n",
            "           1       0.47      0.72      0.57        50\n",
            "\n",
            "    accuracy                           0.46       100\n",
            "   macro avg       0.45      0.46      0.42       100\n",
            "weighted avg       0.45      0.46      0.42       100\n",
            "\n",
            "Mean CV F1: 0.6454 ± 0.1101\n",
            "\n",
            "\n",
            "\n",
            "<==========> Embedding method: fasttext <==========>\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.62      0.60        50\n",
            "           1       0.60      0.56      0.58        50\n",
            "\n",
            "    accuracy                           0.59       100\n",
            "   macro avg       0.59      0.59      0.59       100\n",
            "weighted avg       0.59      0.59      0.59       100\n",
            "\n",
            "Mean CV F1: 0.6349 ± 0.0835\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<==========> Embedding method: bert <==========>\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.60      0.59        50\n",
            "           1       0.58      0.56      0.57        50\n",
            "\n",
            "    accuracy                           0.58       100\n",
            "   macro avg       0.58      0.58      0.58       100\n",
            "weighted avg       0.58      0.58      0.58       100\n",
            "\n",
            "Mean CV F1: 0.6412 ± 0.1229\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}