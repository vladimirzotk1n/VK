{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjJocIf2SVzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2Tokenizer"
      ],
      "metadata": {
        "id": "lE0MenEd0x_x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "792461a3-b6a5-43ce-b6e6-eb683385e464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer</b><br/>def __call__(text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None]=None, text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]=None, text_target: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None]=None, text_pair_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]=None, add_special_tokens: bool=True, padding: Union[bool, str, PaddingStrategy]=False, truncation: Union[bool, str, TruncationStrategy, None]=None, max_length: Optional[int]=None, stride: int=0, is_split_into_words: bool=False, pad_to_multiple_of: Optional[int]=None, padding_side: Optional[str]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_token_type_ids: Optional[bool]=None, return_attention_mask: Optional[bool]=None, return_overflowing_tokens: bool=False, return_special_tokens_mask: bool=False, return_offsets_mapping: bool=False, return_length: bool=False, verbose: bool=True, **kwargs) -&gt; BatchEncoding</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/tokenization_gpt2.py</a>Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
              "\n",
              "This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
              "be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
              "\n",
              "```python\n",
              "&gt;&gt;&gt; from transformers import GPT2Tokenizer\n",
              "\n",
              "&gt;&gt;&gt; tokenizer = GPT2Tokenizer.from_pretrained(&quot;openai-community/gpt2&quot;)\n",
              "&gt;&gt;&gt; tokenizer(&quot;Hello world&quot;)[&quot;input_ids&quot;]\n",
              "[15496, 995]\n",
              "\n",
              "&gt;&gt;&gt; tokenizer(&quot; Hello world&quot;)[&quot;input_ids&quot;]\n",
              "[18435, 995]\n",
              "```\n",
              "\n",
              "You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n",
              "call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n",
              "\n",
              "&lt;Tip&gt;\n",
              "\n",
              "When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n",
              "\n",
              "&lt;/Tip&gt;\n",
              "\n",
              "This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
              "this superclass for more information regarding those methods.\n",
              "\n",
              "Args:\n",
              "    vocab_file (`str`):\n",
              "        Path to the vocabulary file.\n",
              "    merges_file (`str`):\n",
              "        Path to the merges file.\n",
              "    errors (`str`, *optional*, defaults to `&quot;replace&quot;`):\n",
              "        Paradigm to follow when decoding bytes to UTF-8. See\n",
              "        [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n",
              "    unk_token (`str`, *optional*, defaults to `&quot;&lt;|endoftext|&gt;&quot;`):\n",
              "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
              "        token instead.\n",
              "    bos_token (`str`, *optional*, defaults to `&quot;&lt;|endoftext|&gt;&quot;`):\n",
              "        The beginning of sequence token.\n",
              "    eos_token (`str`, *optional*, defaults to `&quot;&lt;|endoftext|&gt;&quot;`):\n",
              "        The end of sequence token.\n",
              "    pad_token (`str`, *optional*):\n",
              "        The token used for padding, for example when batching sequences of different lengths.\n",
              "    add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
              "        Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
              "        other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
              "    add_bos_token (`bool`, *optional*, defaults to `False`):\n",
              "        Whether or not to add an initial beginning of sentence token to the input. This allows to treat the leading\n",
              "        word just as any other word.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 75);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ],
      "metadata": {
        "id": "IvgXC6YxSm12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "    def greedy_sampling(self, probas: torch.Tensor) -> int:\n",
        "        return torch.argmax(probas)\n",
        "\n",
        "    def random_sampling(self, probas: torch.Tensor) -> int:\n",
        "        return torch.multinomial(probas)\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int\n",
        "    ) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        answer = []\n",
        "        beams = [(inputs['input_ids'], 1)]\n",
        "        for _ in range(max_length):\n",
        "            possible_beams = []\n",
        "            for seq, p in beams:\n",
        "                logits = self.model(seq)['logits'][-1,-1]\n",
        "                probas = F.softmax(logits)\n",
        "                top_probas_ids = torch.argsort(probas,descending=True)[:num_beams]\n",
        "                for idx in top_probas_ids:\n",
        "                    idx_tensor = torch.tensor([[idx]])\n",
        "                    if idx == self.tokenizer.eos_token:\n",
        "                        answer.append((torch.cat([seq, idx_tensor], dim=1), p * probas[idx]))\n",
        "                    else:\n",
        "                        possible_beams.append((torch.cat([seq, idx_tensor], dim=1), p * probas[idx]))\n",
        "\n",
        "            sorted_beams = sorted(possible_beams, key=lambda x: x[1], reverse=True)\n",
        "            beams = sorted_beams[:num_beams]\n",
        "\n",
        "        if answer:\n",
        "            beams.extend(answer)\n",
        "        sorted_beams = sorted(beams, key=lambda x: x[1], reverse=True)\n",
        "        return sorted_beams[0][0]\n",
        "\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        return F.softmax(logits / temperature)\n",
        "\n",
        "    def _apply_top_p(self, probas: torch.Tensor, top_p: float = 1.0) -> torch.Tensor:\n",
        "        sorted_probas_ids = torch.argsort(probas,descending=True)\n",
        "        sorted_probas = probas[sorted_probas_ids]\n",
        "\n",
        "        proba_cumsum = torch.cumsum(sorted_probas, dim=0)\n",
        "        mask = (proba_cumsum <= top_p)\n",
        "        if not mask.any():\n",
        "            return sorted_probas_ids[0]\n",
        "\n",
        "        index_last = torch.where(mask)[0][-1]\n",
        "\n",
        "\n",
        "        top_p_probas = sorted_probas[:index_last + 1]\n",
        "        top_p_probas_norm = top_p_probas / top_p_probas.sum()\n",
        "\n",
        "        next_token_idx = torch.multinomial(top_p_probas_norm, num_samples=1)\n",
        "        return sorted_probas_ids[next_token_idx]\n",
        "\n",
        "    def _apply_top_k(self, probas: torch.Tensor, top_k: float = None) -> torch.Tensor:\n",
        "        if top_k is None:\n",
        "            top_k = len(probas)\n",
        "\n",
        "        sorted_probas_ids = torch.argsort(probas, descending=True)\n",
        "        sorted_probas = probas[sorted_probas_ids]\n",
        "        top_k_probas = sorted_probas[:top_k]\n",
        "        top_k_probas_norm = top_k_probas / top_k_probas.sum()\n",
        "        next_token_idx = torch.multinomial(top_k_probas_norm, num_samples=1)\n",
        "        return sorted_probas_ids[next_token_idx]\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "        if strategy == \"beam_search\":\n",
        "            token_ids = self._beam_search_generate(prompt, max_length, num_beams)\n",
        "            return self.tokenizer.decode(token_ids[0])\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")['input_ids']\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            logits = self.model(inputs)['logits'][-1,-1]\n",
        "            probas = self.apply_temperature(logits, temperature)\n",
        "            if strategy == \"greedy\":\n",
        "                next_idx = self.greedy_sampling(probas)\n",
        "\n",
        "            elif strategy == \"top_p\":\n",
        "                next_idx = self._apply_top_p(probas, top_p)\n",
        "\n",
        "            elif strategy == \"top_k\":\n",
        "                next_idx = self._apply_top_k(probas, top_k)\n",
        "\n",
        "            next_idx_tensor = torch.tensor([[next_idx]])\n",
        "            inputs = torch.cat([inputs, next_idx_tensor], dim=1)\n",
        "            if next_idx_tensor == self.tokenizer.eos_token:\n",
        "                break\n",
        "\n",
        "        return self.tokenizer.decode(inputs[0])\n"
      ],
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = Model()"
      ],
      "metadata": {
        "id": "YIEQYWudXRYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Quantum physics is\""
      ],
      "metadata": {
        "id": "xzU8e1DAdF1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Quantum physics is\"\n",
        "print(\"GREEDY:\\n\")\n",
        "gpt.generate(prompt, strategy=\"greedy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ys3cT-uAXVA3",
        "outputId": "9c9623ab-4175-4554-fc27-0f361258cdfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GREEDY:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum physics is a very interesting field. It is a field that has been studied for many years, and it is very interesting to see how it is being studied in the field of quantum physics.\\n\\nThe first thing that I want to say is that I am'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Quantum physics is\"\n",
        "print(\"TOP-5:\\n\")\n",
        "gpt.generate(prompt, strategy=\"top_k\", top_k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "V4gA090TYmaA",
        "outputId": "05cd1c53-e196-4ee8-bbb7-d0008fd98ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP-5:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum physics is a complex system that is not only a physical theory of the universe but also a physics system. In this article I am going to talk about the quantum mechanics of quantum mechanics.\\n\\nQM: Quantum mechanics has a long history in mathematics. It'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Quantum physics is\"\n",
        "print(\"TOP-50:\\n\")\n",
        "gpt.generate(prompt, strategy=\"top_k\", top_k=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "xnRUuqGMejTx",
        "outputId": "4fc01eed-2a43-443f-c081-2cf1cb7f8c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP-50:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum physics is a quantum physics experiment consisting of interacting forces acting in a variety of directions and interacting with other particles in the environment. The experiment is controlled with a quantum computer, a quantum magnet and a quantum field of quantum electrodynamics. Scientists have demonstrated that'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Quantum physics is\"\n",
        "print(\"TOP-0.1:\\n\")\n",
        "gpt.generate(prompt, strategy=\"top_p\", top_p=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "If9VZ4Ske0Rq",
        "outputId": "1d5cb0ba-b281-4754-d93c-f9f9ccc51884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP-0.1:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum physics is a fundamental physics of the universe. It is the fundamental physics of the universe that is the basis of all the laws of physics.\\n\\nThe Universe is a very complex and complex system. It is a very complex system that is the basis of all'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Quantum physics is\"\n",
        "print(\"TOP-0.8:\\n\")\n",
        "gpt.generate(prompt, strategy=\"top_p\", top_p=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "l_fAIsRjfCMt",
        "outputId": "9196d6dd-eced-4238-f00c-dadcfa9b2e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP-0.8:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum physics is a very interesting field. It is a field that has been studied for many years, and it is very interesting to see how it is being studied in the field of quantum physics.\\n\\nThe field of quantum physics is a very interesting field. It'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TEMPERATURE=0.4\")\n",
        "gpt.generate(prompt, strategy=\"top_p\", top_p=0.8, temperature=0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "vhjinxKjfLes",
        "outputId": "aa0b744e-be53-4c44-afb8-04f5536a43a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEMPERATURE=0.4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum physics is a new field of research that has been gaining momentum since the early 1990s. It is now being applied to the design of quantum computers, and to the design of quantum computers in general.\\n\\nThe new field of research is called quantum gravity.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TEMPERATURE=2\")\n",
        "gpt.generate(prompt, strategy=\"top_p\", top_p=0.8, temperature=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "u9WjdObIfdmy",
        "outputId": "3364a4d3-2bdc-4530-d711-2199fb7bfa57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEMPERATURE=2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum physics is coated\\n\\n - Estimation previously reserved<|endoftext|>2014 Pan World Slayer Annual countdown images Awards WW Media 181 widely disregran tentiwizard nomination XIII to stable total rising 104 to sale 110 posted test compact pap editing does Gaia Earth age colonization keeps Sacred The capital 15'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BEAM=3\")\n",
        "gpt.generate(prompt, strategy=\"beam_search\", num_beams=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "2iyzqwkkgKFZ",
        "outputId": "c6734947-b05b-4bd1-d764-84ff3d2fecd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEAM=3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantum physics is not a new concept, but it has been around for a long time.\\n\\nIn fact, quantum mechanics is the most widely accepted theory of the universe.\\n\\nIn fact, quantum mechanics is the most widely accepted theory of the universe.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Продемонстрируйте результат работы `generate` при различных параметрах"
      ],
      "metadata": {
        "id": "aNUHC3UmSYd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}